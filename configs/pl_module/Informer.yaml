_target_: src.pl_modules.pl_baselines.TSForecastTask

name: informer
optimizer:
  _target_: torch.optim.AdamW
  _partial_: true
  lr: 0.0016
  weight_decay: 0.05
  betas: [0.9, 0.95]

scheduler:
  _target_: torch.optim.lr_scheduler.ReduceLROnPlateau
  _partial_: true
  mode: min
  factor: 0.5
  patience: 10

model:
  _target_: src.models.Informer.Model
  enc_in: 3 # Input size of encoder
  dec_in: 3 # Input size of decoder
  c_out: 3 # Output size
  seq_len: 24 # 
  label_len: 12 # Length of input sequence
  pred_len: 24 # Length of prediction sequence
  d_model: 256 # Dimension of the model
  n_heads: 8 # Number of heads
  e_layers: 3 # Number of encoder layers
  d_layers: 2 # Number of decoder layers
  d_ff: 2048 # Dimension of FCN
  factor: 5 # ProbSparse Attention factor
  dropout: 0.05 # Dropout probability
  embed: timeF # Type of time features encoding [timeF, fixed, learned]
  activation: gelu # Activation function 
  freq: h # Frequency of the time series
  distil: True # Whether to use distilling in encoder

metrics:
  train:
    rmse:
      _target_: torchmetrics.MeanSquaredError
      squared: False
    mae:
      _target_: torchmetrics.MeanAbsoluteError
    mape:
      _target_: torchmetrics.MeanAbsolutePercentageError
      # epsilon: 1
  val:
    rmse:
      _target_: torchmetrics.MeanSquaredError
      squared: False
    mae:
      _target_: torchmetrics.MeanAbsoluteError
    mape:
      _target_: torchmetrics.MeanAbsolutePercentageError
      # epsilon: 1
criterion: 
  _target_: torch.nn.MSELoss
monitor: val/rmse

seq_len: 24
label_len: 12
pred_len: 24
padding: 0
inverse_scaling: False
output_attention: False
scaler: None
  