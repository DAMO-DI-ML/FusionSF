# @package _global_

# to execute this experiment run:
# python train.py experiment=example

defaults:
  - override /datamodule: ts_datamodule.yaml
  - override /pl_module: FiLM.yaml #FiLM.yaml, Pyraformer.yaml, PatchTST.yaml, Nonstationary_Transformer.yaml, TimesNet.yaml, Reformer.yaml, MICN.yaml, LightTS.yaml, FEDformer.yaml, ETSformer.yaml, DLinear.yaml, Transformer.yaml, Autoformer.yaml
  - override /trainer: default.yaml

# all parameters below will be merged with parameters from default configurations set above
# this allows you to overwrite only specified parameters
# NEED TO DEBUG THIS: Pyraformer.yaml, Nonstationary_Transformer.yaml, MICN.yaml ETSformer.yaml
# ADAPT pl.module: Reformer.yaml, 

seed: 42

pl_module:
  model:
    enc_in: 3
    seq_len: 24
    label_len: 12
    pred_len: 24
    e_layers: 2
    ratio: 0.4

trainer:
  max_epochs: 100


logger:
  wandb:
    group: "baseline"